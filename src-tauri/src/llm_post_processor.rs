// src-tauri/src/llm_post_processor.rs
//
// LLM 文本润色处理模块
//
// 基于通用 OpenAI 客户端，提供文本润色功能
// 支持多预设管理，用户可自定义润色风格

use anyhow::Result;

use crate::config::LlmConfig;
use crate::openai_client::{ChatOptions, OpenAiClient, OpenAiClientConfig};

/// LLM 文本润色处理器
///
/// 使用通用 OpenAI 客户端，专注于文本润色功能
#[derive(Clone)]
pub struct LlmPostProcessor {
    client: OpenAiClient,
    config: LlmConfig,
}

impl LlmPostProcessor {
    /// 创建新的处理器实例
    pub fn new(config: LlmConfig) -> Self {
        let client_config = OpenAiClientConfig::new(
            &config.endpoint,
            &config.api_key,
            &config.model,
        );
        let client = OpenAiClient::new(client_config);

        Self { client, config }
    }

    /// 获取当前激活的润色 Prompt
    fn get_active_system_prompt(&self) -> String {
        self.config
            .presets
            .iter()
            .find(|p| p.id == self.config.active_preset_id)
            .map(|p| p.system_prompt.clone())
            .unwrap_or_else(|| "You are a helpful assistant.".to_string())
    }

    /// 文本润色
    ///
    /// 使用当前激活的预设对 ASR 转写文本进行润色
    ///
    /// # Arguments
    /// * `raw_text` - ASR 转写的原始文本
    ///
    /// # Returns
    /// * 润色后的文本
    pub async fn polish_transcript(&self, raw_text: &str) -> Result<String> {
        if raw_text.trim().is_empty() {
            return Ok(String::new());
        }

        let system_prompt = self.get_active_system_prompt();
        tracing::info!("LLM 润色使用预设 ID: {}", self.config.active_preset_id);

        // 添加明确的标识符，防止模型误判为提问
        let user_message = format!("<用户ASR的转写内容>\n\n{}\n\n</用户ASR的转写内容>", raw_text);

        self.client
            .chat_simple(&system_prompt, &user_message, ChatOptions::for_polishing())
            .await
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use crate::config::LlmPreset;

    fn create_test_config() -> LlmConfig {
        LlmConfig {
            endpoint: "https://api.example.com/v1/chat/completions".to_string(),
            model: "test-model".to_string(),
            api_key: "test-key".to_string(),
            presets: vec![
                LlmPreset {
                    id: "test".to_string(),
                    name: "Test Preset".to_string(),
                    system_prompt: "You are a test assistant.".to_string(),
                },
            ],
            active_preset_id: "test".to_string(),
        }
    }

    #[test]
    fn test_get_active_system_prompt() {
        let config = create_test_config();
        let processor = LlmPostProcessor::new(config);
        let prompt = processor.get_active_system_prompt();
        assert_eq!(prompt, "You are a test assistant.");
    }

    #[test]
    fn test_get_active_system_prompt_fallback() {
        let mut config = create_test_config();
        config.active_preset_id = "non-existent".to_string();
        let processor = LlmPostProcessor::new(config);
        let prompt = processor.get_active_system_prompt();
        assert_eq!(prompt, "You are a helpful assistant.");
    }
}
